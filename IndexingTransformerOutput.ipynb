{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "IndexingTransformerOutput.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNce9EAF8k4WPO5qsPdxSni",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5JpojCAHKqS",
    "outputId": "a2ac13d5-646e-4baa-8fab-78b66c4d1834"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "\u001B[K     |████████████████████████████████| 3.5 MB 13.9 MB/s \n",
      "\u001B[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "\u001B[K     |████████████████████████████████| 6.8 MB 11.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001B[K     |████████████████████████████████| 895 kB 40.8 MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001B[K     |████████████████████████████████| 67 kB 3.1 MB/s \n",
      "\u001B[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001B[K     |████████████████████████████████| 596 kB 18.5 MB/s \n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Toywutn5GT2J"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = 50257"
   ],
   "metadata": {
    "id": "8lq_LGok34Tf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sample_input = tokenizer.encode(\" But if you are preparing data and doing cat in each iteration, it gets really slow when the tensor you are generating gets very large. My solution was to cat into\", return_tensors=\"pt\")[0]\n",
    "sample_input"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3un9wjCIQAh",
    "outputId": "ad6c4d1b-a8f2-4d78-ea2e-c644382beb47"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([  887,   611,   345,   389, 10629,  1366,   290,  1804,  3797,   287,\n",
       "         1123, 24415,    11,   340,  3011,  1107,  3105,   618,   262, 11192,\n",
       "          273,   345,   389, 15453,  3011,   845,  1588,    13,  2011,  4610,\n",
       "          373,   284,  3797,   656])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def valid_encoding(shifted_input, encoded_msg, sorted_tokens):\n",
    "  # At each timestep, use the encoded message to select the tokens at the specified\n",
    "  # index of the list of sorted tokens to reconstruct the original message.\n",
    "  # Compare against the original message to ensure they are identical.\n",
    "  msg_len = encoded_msg.size()[0]\n",
    "\n",
    "  # Flatten the tensor of sorted tokens to make indexing easier\n",
    "  # and add offsets to the encoded message to account for this flattening\n",
    "  vocab_size = sorted_tokens.size()[1]\n",
    "  sorted_tokens_flat = torch.flatten(sorted_tokens)\n",
    "  encoded_msg_offset = encoded_msg + torch.arange(0,vocab_size*msg_len,vocab_size)\n",
    "  decoded_msg_cand = torch.index_select(sorted_tokens_flat, 0, encoded_msg_offset)\n",
    "  return torch.all(decoded_msg_cand == shifted_input[:-1])\n",
    "  \n",
    "\n",
    "def encode(tokenized_msg, vocab_size):\n",
    "  # Encode\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    # In theory, I should be able to avoid the loop because the transformer\n",
    "    # automatically masks the input. But in practice, this causes the logit\n",
    "    # outputs to differ slightly between the encoder and decoder\n",
    "    msg_len = tokenized_msg.size()[0]\n",
    "    logits_arr = torch.zeros(msg_len, vocab_size)\n",
    "    for i in range(msg_len):\n",
    "      msg_slice = tokenized_msg[:i+1]\n",
    "      logits = model(msg_slice).logits\n",
    "      logits_arr[i] = logits[i]\n",
    "    \n",
    "  # Sort the indices of the logits in descending order of logit value.\n",
    "  # This means that the model's top predicted token is the first\n",
    "  # element in the sorted list, the second highest predicted token is the \n",
    "  # second element, and so on.\n",
    "  # \n",
    "  # Once we have this list of tokens ordered by their probability\n",
    "  # we can find the ground-truth token in this list, and save its index\n",
    "  # as the encoding of the token.\n",
    "  shifted_input = torch.roll(sample_input, -1) # Shift input to line up with output\n",
    "  _, sorted_tokens = torch.sort(logits_arr, dim=1, descending=True, stable=True)\n",
    "  encoded_msg = (sorted_tokens == shifted_input.view(-1, 1)).nonzero()[:,1]\n",
    "  encoded_msg = encoded_msg[:-1] # Discard the last index because it overflows the original message\n",
    "  assert valid_encoding(shifted_input, encoded_msg, sorted_tokens)\n",
    "\n",
    "  return encoded_msg, logits_arr # Logits for debugging\n",
    "\n",
    "def decode(encoded_msg, first_token, vocab_size):\n",
    "  with torch.no_grad():\n",
    "    msg_len = encoded_msg.size()[0]\n",
    "    logits_arr = torch.zeros(msg_len, vocab_size) # For debugging\n",
    "    decoded_msg = first_token\n",
    "    for i in range(encoded_msg.size()[0]):\n",
    "      logits = model(decoded_msg).logits\n",
    "      logits_arr[i] = logits[i] # For debugging\n",
    "      _, indices = torch.sort(logits[i], dim=0, descending=True, stable=True)\n",
    "      decoded_token = indices[encoded_msg[i:i+1]]\n",
    "      decoded_msg = torch.cat((decoded_msg, decoded_token))\n",
    "  return decoded_msg, logits_arr # Logits for debugging"
   ],
   "metadata": {
    "id": "7KwhY2CrH1j5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "original_msg = sample_input\n",
    "encoded_msg, encoder_logits = encode(original_msg, VOCAB_SIZE)\n",
    "decoded_msg, decoder_logits = decode(encoded_msg, original_msg[:1],VOCAB_SIZE)\n",
    "# Encoder and decoder logits should be identical\n",
    "assert torch.all(encoder_logits[0:-1] == decoder_logits)\n",
    "# Decoded message and original message should be identical\n",
    "assert torch.all(decoded_msg == original_msg)\n"
   ],
   "metadata": {
    "id": "-fYjllYkwE2U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}